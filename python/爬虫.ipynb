{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备扒取：岛国痴女小岛美优高清写真，少妇一般欲望难挡\n",
      "https://i3.mmzztt.com/2020/02/25c01.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c02.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c03.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c04.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c05.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c06.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c07.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c08.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c09.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c10.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c11.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c12.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c13.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c14.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c15.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c16.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c17.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c18.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c19.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c20.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c21.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c22.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c23.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c24.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c25.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c26.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c27.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c28.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c29.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c30.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c31.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c32.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c33.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c34.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c35.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c36.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c37.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c38.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c39.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c40.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c41.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c42.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c43.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c44.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c45.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c46.jpg\n",
      "https://i3.mmzztt.com/2020/02/25c47.jpg\n",
      "完成\n",
      "准备扒取：日本女优吉见早央高清写真 撅臀开腿散发妩媚女人味\n",
      "https://i3.mmzztt.com/2019/12/17a01.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a52.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a02.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a03.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a04.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a05.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a06.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a07.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a08.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a09.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a10.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a11.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a12.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a13.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a14.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a15.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a16.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a17.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a18.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a19.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a20.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a21.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a22.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a23.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a24.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a25.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a26.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a27.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a28.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a29.jpg\n",
      "https://i3.mmzztt.com/2019/12/17a30.jpg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6165c93a7174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mmess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mpic_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic_url\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m                 \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic_url\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPicreferer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    " \n",
    "all_url = 'https://www.mzitu.com'\n",
    " \n",
    "# http请求头\n",
    "Hostreferer = {\n",
    "    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)',\n",
    "    'Referer': 'http://www.mzitu.com'\n",
    "}\n",
    "# 此请求头Referer破解盗图链接\n",
    "Picreferer = {\n",
    "    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)',\n",
    "    'Referer': 'https://www.mzitu.com'\n",
    "}\n",
    " \n",
    "# 对mzitu主页all_url发起请求，将返回的HTML数据保存，便于解析\n",
    "start_html = requests.get(all_url, headers=Hostreferer)\n",
    " \n",
    "# Linux保存地址\n",
    "# path = '/home/Nick/Desktop/mzitu/'\n",
    " \n",
    "# Windows保存地址\n",
    "path = 'D:/mzitu/'\n",
    " \n",
    "# 获取最大页数\n",
    "soup = BeautifulSoup(start_html.text, \"html.parser\")\n",
    "page = soup.find_all('a', class_='page-numbers')\n",
    "max_page = page[-2].text\n",
    " \n",
    " \n",
    "# same_url = 'http://www.mzitu.com/page/'   # 主页默认最新图片\n",
    "# 获取每一类MM的网址\n",
    "same_url = 'https://www.mzitu.com/japan/page/'     # 也可以指定《qingchun MM系列》\n",
    " \n",
    "for n in range(1, int(max_page) + 1):\n",
    "    # 拼接当前类MM的所有url\n",
    "    ul = same_url + str(n)\n",
    " \n",
    "    # 分别对当前类每一页第一层url发起请求\n",
    "    start_html = requests.get(ul, headers=Hostreferer)\n",
    " \n",
    "    # 提取所有MM的标题\n",
    "    soup = BeautifulSoup(start_html.text, \"html.parser\")\n",
    "    all_a = soup.find('div', class_='postlist').find_all('a', target='_blank')\n",
    " \n",
    "    # 遍历所有MM的标题\n",
    "    for a in all_a:\n",
    "        # 提取标题文本，作为文件夹名称\n",
    "        title = a.get_text()\n",
    "        if(title != ''):\n",
    "            print(\"准备扒取：\" + title)\n",
    " \n",
    "            # windows不能创建带？的目录，添加判断逻辑\n",
    "            if(os.path.exists(path + title.strip().replace('?', ''))):\n",
    "                # print('目录已存在')\n",
    "                flag = 1\n",
    "            else:\n",
    "                os.makedirs(path + title.strip().replace('?', ''))\n",
    "                flag = 0\n",
    "            # 切换到上一步创建的目录\n",
    "            os.chdir(path + title.strip().replace('?', ''))\n",
    " \n",
    "            # 提取第一层每一个MM的url，并发起请求\n",
    "            href = a['href']\n",
    "            html = requests.get(href, headers=Hostreferer)\n",
    "            mess = BeautifulSoup(html.text, \"html.parser\")\n",
    " \n",
    "            # 获取第二层最大页数\n",
    "            pic_max = mess.find_all('span')\n",
    "            pic_max = pic_max[9].text\n",
    "            if(flag == 1 and len(os.listdir(path + title.strip().replace('?', ''))) >= int(pic_max)):\n",
    "                print('已经保存完毕，跳过')\n",
    "                continue\n",
    " \n",
    "            # 遍历第二层每张图片的url\n",
    "            for num in range(1, int(pic_max) + 1):\n",
    "                # 拼接每张图片的url\n",
    "                pic = href + '/' + str(num)\n",
    " \n",
    "                # 发起请求\n",
    "                html = requests.get(pic, headers=Hostreferer)\n",
    "                mess = BeautifulSoup(html.text, \"html.parser\")\n",
    "                pic_url = mess.find('img', alt=title)\n",
    "                print(pic_url['src'])\n",
    "                html = requests.get(pic_url['src'], headers=Picreferer)\n",
    " \n",
    "                # 提取图片名字\n",
    "                file_name = pic_url['src'].split(r'/')[-1]\n",
    " \n",
    "                # 保存图片\n",
    "                f = open(file_name, 'wb')\n",
    "                f.write(html.content)\n",
    "                f.close()\n",
    "            print('完成')\n",
    "    print('第', n, '页完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取第1页\n",
      "正在爬取第2页\n",
      "正在爬取第3页\n",
      "正在爬取第4页\n",
      "正在爬取第5页\n",
      "正在爬取第6页\n",
      "正在爬取第7页\n",
      "正在爬取第8页\n",
      "正在爬取第9页\n",
      "正在爬取第10页\n",
      "正在爬取第11页\n",
      "正在爬取第12页\n",
      "正在爬取第13页\n",
      "正在爬取第14页\n",
      "正在爬取第15页\n",
      "正在爬取第16页\n",
      "正在爬取第17页\n",
      "正在爬取第18页\n",
      "正在爬取第19页\n",
      "正在爬取第20页\n",
      "正在爬取第21页\n",
      "正在爬取第22页\n",
      "正在爬取第23页\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "#作者:猫先生的早茶\n",
    "#时间:2019年5月19日\n",
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "设置浏览器头部，\n",
    "User-Agent用于表示浏览器的参数信息\n",
    "Referer用于设置使用那个网页跳转过来的\n",
    "url用于设置网址模板，可以通过.format参数补充网址\n",
    "\"\"\"\n",
    "header = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "          \"Referer\":\"https://www.mzitu.com/jiepai/comment-page-1/\",}\n",
    "url = 'https://www.mzitu.com/jiepai/comment-page-{}/#comments'\n",
    "name = 0\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"获取网页代码并以返回值的形式弹出\"\"\"\n",
    "    html = requests.get(url,headers=header).text\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_img(url):\n",
    "    \"\"\"下载图片并保存到指定文件夹下\"\"\"\n",
    "    global name\n",
    "    name +=1\n",
    "    img_name = 'D:/mzitu/{}.jpg'.format(name)\n",
    "    img = requests.get(url,headers=header).content\n",
    "    with open (img_name,'wb') as save_img:\n",
    "        save_img.write(img)\n",
    "\n",
    "def get_url(html):\n",
    "    \"\"\"获取图片链接并以返回值的形式弹出\"\"\"\n",
    "    etree_html = etree.HTML(html)\n",
    "    img_url = etree_html.xpath('//img[@class=\"lazy\"]/@data-original')\n",
    "    return img_url\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''使用for循环爬取所有网页'''\n",
    "    for n in range(1,24):\n",
    "        print (\"正在爬取第{}页\".format(n))\n",
    "        html = get_html(url.format(n))\n",
    "        img_list = get_url(html)\n",
    "        for img in img_list:\n",
    "            get_img(img)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-61759961281f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmain_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'main-image'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mmeizitu_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'img'"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- encoding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 定制请求头\n",
    "headers = {'Referer':'https://www.mzitu.com','User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3679.0 Safari/537.36'}\n",
    "\n",
    "path = 'D:/meizitu'\n",
    "meizi_url = []\n",
    "meizitu_img = []\n",
    "\n",
    "start_url = 'https://www.mzitu.com/177007'\n",
    "meizi_url.append(start_url)\n",
    "r = requests.get(start_url)\n",
    "soup = BeautifulSoup(r.text)\n",
    "main_img = soup.find('div', 'main-image').img.get('src')\n",
    "meizitu_img.append(main_img)\n",
    "\n",
    "guess_like = soup.find('dl', 'widgets_like').find_all('a')\n",
    "for a in guess_like:\n",
    "    meizi_url.append(a.get('href'))\n",
    "# 删除起始引导url\n",
    "# del meizi_url[0]\n",
    "\n",
    "# print(meizi_url)\n",
    "# print(meizitu_img)\n",
    "with open(\"D:/meizitu/meizi-main-jpg.txt\", \"w\") as fo:\n",
    "    x = 1\n",
    "    y = 1\n",
    "    for node_url in meizi_url:\n",
    "        r = requests.get(node_url)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        main_img = soup.find('div', 'main-image').img.get('src')\n",
    "        # 添加到文件日志并下载主图\n",
    "        if main_img not in meizitu_img:\n",
    "            x += 1\n",
    "            meizitu_img.append(main_img)\n",
    "            # 写入日志\n",
    "            fo.write(main_img+'\\n')\n",
    "            # 下载主图\n",
    "            res = requests.get(main_img, headers=headers)\n",
    "            if res.status_code == 200:\n",
    "                with open(path+str(x)+'-'+str(y)+'.jpg', 'wb') as f:\n",
    "                    f.write(res.content)\n",
    "                    print('成功保存图片')  \n",
    "        # 猜你喜欢，跳转其他页面\n",
    "        guess_like = soup.find('dl', 'widgets_like').find_all('a')\n",
    "        for a in guess_like:\n",
    "            like = a.get('href')\n",
    "            # 添加推荐页面\n",
    "            if like not in meizi_url:\n",
    "                y += 1\n",
    "                meizi_url.append(like)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
